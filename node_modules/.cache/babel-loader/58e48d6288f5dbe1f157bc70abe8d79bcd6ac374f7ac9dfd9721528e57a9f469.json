{"ast":null,"code":"// File generated from our OpenAPI spec by Stainless.\nimport { APIResource } from 'openai/resource';\nimport * as FineTunesAPI from 'openai/resources/fine-tunes';\nimport { Page } from 'openai/pagination';\nexport class FineTunes extends APIResource {\n  /**\n   * Creates a job that fine-tunes a specified model from a given dataset.\n   *\n   * Response includes details of the enqueued job including job status and the name\n   * of the fine-tuned models once complete.\n   *\n   * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/legacy-fine-tuning)\n   */\n  create(body, options) {\n    return this._client.post('/fine-tunes', {\n      body,\n      ...options\n    });\n  }\n  /**\n   * Gets info about the fine-tune job.\n   *\n   * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/legacy-fine-tuning)\n   */\n  retrieve(fineTuneId, options) {\n    return this._client.get(`/fine-tunes/${fineTuneId}`, options);\n  }\n  /**\n   * List your organization's fine-tuning jobs\n   */\n  list(options) {\n    return this._client.getAPIList('/fine-tunes', FineTunesPage, options);\n  }\n  /**\n   * Immediately cancel a fine-tune job.\n   */\n  cancel(fineTuneId, options) {\n    return this._client.post(`/fine-tunes/${fineTuneId}/cancel`, options);\n  }\n  listEvents(fineTuneId, query, options) {\n    return this._client.get(`/fine-tunes/${fineTuneId}/events`, {\n      query,\n      timeout: 86400000,\n      ...options,\n      stream: query?.stream ?? false\n    });\n  }\n}\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nexport class FineTunesPage extends Page {}\n(function (FineTunes) {\n  FineTunes.FineTunesPage = FineTunesAPI.FineTunesPage;\n})(FineTunes || (FineTunes = {}));","map":{"version":3,"names":["APIResource","FineTunesAPI","Page","FineTunes","create","body","options","_client","post","retrieve","fineTuneId","get","list","getAPIList","FineTunesPage","cancel","listEvents","query","timeout","stream"],"sources":["C:\\Users\\vkdld\\OneDrive\\바탕 화면\\opensource2\\newsproject\\node_modules\\openai\\src\\resources\\fine-tunes.ts"],"sourcesContent":["// File generated from our OpenAPI spec by Stainless.\n\nimport * as Core from \"../core\";\nimport { APIPromise } from \"../core\";\nimport { APIResource } from \"../resource\";\nimport * as FineTunesAPI from \"./fine-tunes\";\nimport * as FilesAPI from \"./files\";\nimport { Page } from \"../pagination\";\nimport { Stream } from \"../streaming\";\n\nexport class FineTunes extends APIResource {\n  /**\n   * Creates a job that fine-tunes a specified model from a given dataset.\n   *\n   * Response includes details of the enqueued job including job status and the name\n   * of the fine-tuned models once complete.\n   *\n   * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/legacy-fine-tuning)\n   */\n  create(body: FineTuneCreateParams, options?: Core.RequestOptions): Core.APIPromise<FineTune> {\n    return this._client.post('/fine-tunes', { body, ...options });\n  }\n\n  /**\n   * Gets info about the fine-tune job.\n   *\n   * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/legacy-fine-tuning)\n   */\n  retrieve(fineTuneId: string, options?: Core.RequestOptions): Core.APIPromise<FineTune> {\n    return this._client.get(`/fine-tunes/${fineTuneId}`, options);\n  }\n\n  /**\n   * List your organization's fine-tuning jobs\n   */\n  list(options?: Core.RequestOptions): Core.PagePromise<FineTunesPage, FineTune> {\n    return this._client.getAPIList('/fine-tunes', FineTunesPage, options);\n  }\n\n  /**\n   * Immediately cancel a fine-tune job.\n   */\n  cancel(fineTuneId: string, options?: Core.RequestOptions): Core.APIPromise<FineTune> {\n    return this._client.post(`/fine-tunes/${fineTuneId}/cancel`, options);\n  }\n\n  /**\n   * Get fine-grained status updates for a fine-tune job.\n   */\n  listEvents(\n    fineTuneId: string,\n    query?: FineTuneListEventsParamsNonStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<FineTuneEventsListResponse>;\n  listEvents(\n    fineTuneId: string,\n    query: FineTuneListEventsParamsStreaming,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<FineTuneEvent>>;\n  listEvents(\n    fineTuneId: string,\n    query?: FineTuneListEventsParamsBase | undefined,\n    options?: Core.RequestOptions,\n  ): APIPromise<Stream<FineTuneEvent> | FineTuneEventsListResponse>;\n  listEvents(\n    fineTuneId: string,\n    query?: FineTuneListEventsParams | undefined,\n    options?: Core.RequestOptions,\n  ): APIPromise<FineTuneEventsListResponse> | APIPromise<Stream<FineTuneEvent>> {\n    return this._client.get(`/fine-tunes/${fineTuneId}/events`, {\n      query,\n      timeout: 86400000,\n      ...options,\n      stream: query?.stream ?? false,\n    }) as APIPromise<FineTuneEventsListResponse> | APIPromise<Stream<FineTuneEvent>>;\n  }\n}\n\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nexport class FineTunesPage extends Page<FineTune> {}\n\n/**\n * The `FineTune` object represents a legacy fine-tune job that has been created\n * through the API.\n */\nexport interface FineTune {\n  /**\n   * The object identifier, which can be referenced in the API endpoints.\n   */\n  id: string;\n\n  /**\n   * The Unix timestamp (in seconds) for when the fine-tuning job was created.\n   */\n  created_at: number;\n\n  /**\n   * The name of the fine-tuned model that is being created.\n   */\n  fine_tuned_model: string | null;\n\n  /**\n   * The hyperparameters used for the fine-tuning job. See the\n   * [fine-tuning guide](https://platform.openai.com/docs/guides/legacy-fine-tuning/hyperparameters)\n   * for more details.\n   */\n  hyperparams: FineTune.Hyperparams;\n\n  /**\n   * The base model that is being fine-tuned.\n   */\n  model: string;\n\n  /**\n   * The object type, which is always \"fine-tune\".\n   */\n  object: 'fine-tune';\n\n  /**\n   * The organization that owns the fine-tuning job.\n   */\n  organization_id: string;\n\n  /**\n   * The compiled results files for the fine-tuning job.\n   */\n  result_files: Array<FilesAPI.FileObject>;\n\n  /**\n   * The current status of the fine-tuning job, which can be either `created`,\n   * `running`, `succeeded`, `failed`, or `cancelled`.\n   */\n  status: string;\n\n  /**\n   * The list of files used for training.\n   */\n  training_files: Array<FilesAPI.FileObject>;\n\n  /**\n   * The Unix timestamp (in seconds) for when the fine-tuning job was last updated.\n   */\n  updated_at: number;\n\n  /**\n   * The list of files used for validation.\n   */\n  validation_files: Array<FilesAPI.FileObject>;\n\n  /**\n   * The list of events that have been observed in the lifecycle of the FineTune job.\n   */\n  events?: Array<FineTuneEvent>;\n}\n\nexport namespace FineTune {\n  /**\n   * The hyperparameters used for the fine-tuning job. See the\n   * [fine-tuning guide](https://platform.openai.com/docs/guides/legacy-fine-tuning/hyperparameters)\n   * for more details.\n   */\n  export interface Hyperparams {\n    /**\n     * The batch size to use for training. The batch size is the number of training\n     * examples used to train a single forward and backward pass.\n     */\n    batch_size: number;\n\n    /**\n     * The learning rate multiplier to use for training.\n     */\n    learning_rate_multiplier: number;\n\n    /**\n     * The number of epochs to train the model for. An epoch refers to one full cycle\n     * through the training dataset.\n     */\n    n_epochs: number;\n\n    /**\n     * The weight to use for loss on the prompt tokens.\n     */\n    prompt_loss_weight: number;\n\n    /**\n     * The number of classes to use for computing classification metrics.\n     */\n    classification_n_classes?: number;\n\n    /**\n     * The positive class to use for computing classification metrics.\n     */\n    classification_positive_class?: string;\n\n    /**\n     * The classification metrics to compute using the validation dataset at the end of\n     * every epoch.\n     */\n    compute_classification_metrics?: boolean;\n  }\n}\n\n/**\n * Fine-tune event object\n */\nexport interface FineTuneEvent {\n  created_at: number;\n\n  level: string;\n\n  message: string;\n\n  object: 'fine-tune-event';\n}\n\nexport interface FineTuneEventsListResponse {\n  data: Array<FineTuneEvent>;\n\n  object: 'list';\n}\n\nexport interface FineTuneCreateParams {\n  /**\n   * The ID of an uploaded file that contains training data.\n   *\n   * See [upload file](https://platform.openai.com/docs/api-reference/files/upload)\n   * for how to upload a file.\n   *\n   * Your dataset must be formatted as a JSONL file, where each training example is a\n   * JSON object with the keys \"prompt\" and \"completion\". Additionally, you must\n   * upload your file with the purpose `fine-tune`.\n   *\n   * See the\n   * [fine-tuning guide](https://platform.openai.com/docs/guides/legacy-fine-tuning/creating-training-data)\n   * for more details.\n   */\n  training_file: string;\n\n  /**\n   * The batch size to use for training. The batch size is the number of training\n   * examples used to train a single forward and backward pass.\n   *\n   * By default, the batch size will be dynamically configured to be ~0.2% of the\n   * number of examples in the training set, capped at 256 - in general, we've found\n   * that larger batch sizes tend to work better for larger datasets.\n   */\n  batch_size?: number | null;\n\n  /**\n   * If this is provided, we calculate F-beta scores at the specified beta values.\n   * The F-beta score is a generalization of F-1 score. This is only used for binary\n   * classification.\n   *\n   * With a beta of 1 (i.e. the F-1 score), precision and recall are given the same\n   * weight. A larger beta score puts more weight on recall and less on precision. A\n   * smaller beta score puts more weight on precision and less on recall.\n   */\n  classification_betas?: Array<number> | null;\n\n  /**\n   * The number of classes in a classification task.\n   *\n   * This parameter is required for multiclass classification.\n   */\n  classification_n_classes?: number | null;\n\n  /**\n   * The positive class in binary classification.\n   *\n   * This parameter is needed to generate precision, recall, and F1 metrics when\n   * doing binary classification.\n   */\n  classification_positive_class?: string | null;\n\n  /**\n   * If set, we calculate classification-specific metrics such as accuracy and F-1\n   * score using the validation set at the end of every epoch. These metrics can be\n   * viewed in the\n   * [results file](https://platform.openai.com/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model).\n   *\n   * In order to compute classification metrics, you must provide a\n   * `validation_file`. Additionally, you must specify `classification_n_classes` for\n   * multiclass classification or `classification_positive_class` for binary\n   * classification.\n   */\n  compute_classification_metrics?: boolean | null;\n\n  /**\n   * The hyperparameters used for the fine-tuning job.\n   */\n  hyperparameters?: FineTuneCreateParams.Hyperparameters;\n\n  /**\n   * The learning rate multiplier to use for training. The fine-tuning learning rate\n   * is the original learning rate used for pretraining multiplied by this value.\n   *\n   * By default, the learning rate multiplier is the 0.05, 0.1, or 0.2 depending on\n   * final `batch_size` (larger learning rates tend to perform better with larger\n   * batch sizes). We recommend experimenting with values in the range 0.02 to 0.2 to\n   * see what produces the best results.\n   */\n  learning_rate_multiplier?: number | null;\n\n  /**\n   * The name of the base model to fine-tune. You can select one of \"ada\", \"babbage\",\n   * \"curie\", \"davinci\", or a fine-tuned model created after 2022-04-21 and before\n   * 2023-08-22. To learn more about these models, see the\n   * [Models](https://platform.openai.com/docs/models) documentation.\n   */\n  model?: (string & {}) | 'ada' | 'babbage' | 'curie' | 'davinci' | null;\n\n  /**\n   * The weight to use for loss on the prompt tokens. This controls how much the\n   * model tries to learn to generate the prompt (as compared to the completion which\n   * always has a weight of 1.0), and can add a stabilizing effect to training when\n   * completions are short.\n   *\n   * If prompts are extremely long (relative to completions), it may make sense to\n   * reduce this weight so as to avoid over-prioritizing learning the prompt.\n   */\n  prompt_loss_weight?: number | null;\n\n  /**\n   * A string of up to 40 characters that will be added to your fine-tuned model\n   * name.\n   *\n   * For example, a `suffix` of \"custom-model-name\" would produce a model name like\n   * `ada:ft-your-org:custom-model-name-2022-02-15-04-21-04`.\n   */\n  suffix?: string | null;\n\n  /**\n   * The ID of an uploaded file that contains validation data.\n   *\n   * If you provide this file, the data is used to generate validation metrics\n   * periodically during fine-tuning. These metrics can be viewed in the\n   * [fine-tuning results file](https://platform.openai.com/docs/guides/legacy-fine-tuning/analyzing-your-fine-tuned-model).\n   * Your train and validation data should be mutually exclusive.\n   *\n   * Your dataset must be formatted as a JSONL file, where each validation example is\n   * a JSON object with the keys \"prompt\" and \"completion\". Additionally, you must\n   * upload your file with the purpose `fine-tune`.\n   *\n   * See the\n   * [fine-tuning guide](https://platform.openai.com/docs/guides/legacy-fine-tuning/creating-training-data)\n   * for more details.\n   */\n  validation_file?: string | null;\n}\n\nexport namespace FineTuneCreateParams {\n  /**\n   * The hyperparameters used for the fine-tuning job.\n   */\n  export interface Hyperparameters {\n    /**\n     * The number of epochs to train the model for. An epoch refers to one full cycle\n     * through the training dataset.\n     */\n    n_epochs?: 'auto' | number;\n  }\n}\n\nexport type FineTuneListEventsParams =\n  | FineTuneListEventsParamsNonStreaming\n  | FineTuneListEventsParamsStreaming;\n\nexport interface FineTuneListEventsParamsBase {\n  /**\n   * Whether to stream events for the fine-tune job. If set to true, events will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available. The stream will terminate with a `data: [DONE]`\n   * message when the job is finished (succeeded, cancelled, or failed).\n   *\n   * If set to false, only events generated so far will be returned.\n   */\n  stream?: boolean;\n}\n\nexport namespace FineTuneListEventsParams {\n  export type FineTuneListEventsParamsNonStreaming = FineTunesAPI.FineTuneListEventsParamsNonStreaming;\n  export type FineTuneListEventsParamsStreaming = FineTunesAPI.FineTuneListEventsParamsStreaming;\n}\n\nexport interface FineTuneListEventsParamsNonStreaming extends FineTuneListEventsParamsBase {\n  /**\n   * Whether to stream events for the fine-tune job. If set to true, events will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available. The stream will terminate with a `data: [DONE]`\n   * message when the job is finished (succeeded, cancelled, or failed).\n   *\n   * If set to false, only events generated so far will be returned.\n   */\n  stream?: false;\n}\n\nexport interface FineTuneListEventsParamsStreaming extends FineTuneListEventsParamsBase {\n  /**\n   * Whether to stream events for the fine-tune job. If set to true, events will be\n   * sent as data-only\n   * [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format)\n   * as they become available. The stream will terminate with a `data: [DONE]`\n   * message when the job is finished (succeeded, cancelled, or failed).\n   *\n   * If set to false, only events generated so far will be returned.\n   */\n  stream: true;\n}\n\nexport namespace FineTunes {\n  export import FineTune = FineTunesAPI.FineTune;\n  export import FineTuneEvent = FineTunesAPI.FineTuneEvent;\n  export import FineTuneEventsListResponse = FineTunesAPI.FineTuneEventsListResponse;\n  export import FineTunesPage = FineTunesAPI.FineTunesPage;\n  export import FineTuneCreateParams = FineTunesAPI.FineTuneCreateParams;\n  export import FineTuneListEventsParams = FineTunesAPI.FineTuneListEventsParams;\n  export import FineTuneListEventsParamsNonStreaming = FineTunesAPI.FineTuneListEventsParamsNonStreaming;\n  export import FineTuneListEventsParamsStreaming = FineTunesAPI.FineTuneListEventsParamsStreaming;\n}\n"],"mappings":"AAAA;SAISA,WAAW,QAAQ,iBAAiB;OACtC,KAAKC,YAAY,MAAM,6BAA6B;SAElDC,IAAI,QAAQ,mBAAmB;AAGxC,OAAM,MAAOC,SAAU,SAAQH,WAAW;EACxC;;;;;;;;EAQAI,MAAMA,CAACC,IAA0B,EAAEC,OAA6B;IAC9D,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,aAAa,EAAE;MAAEH,IAAI;MAAE,GAAGC;IAAO,CAAE,CAAC;EAC/D;EAEA;;;;;EAKAG,QAAQA,CAACC,UAAkB,EAAEJ,OAA6B;IACxD,OAAO,IAAI,CAACC,OAAO,CAACI,GAAG,CAAC,eAAeD,UAAU,EAAE,EAAEJ,OAAO,CAAC;EAC/D;EAEA;;;EAGAM,IAAIA,CAACN,OAA6B;IAChC,OAAO,IAAI,CAACC,OAAO,CAACM,UAAU,CAAC,aAAa,EAAEC,aAAa,EAAER,OAAO,CAAC;EACvE;EAEA;;;EAGAS,MAAMA,CAACL,UAAkB,EAAEJ,OAA6B;IACtD,OAAO,IAAI,CAACC,OAAO,CAACC,IAAI,CAAC,eAAeE,UAAU,SAAS,EAAEJ,OAAO,CAAC;EACvE;EAoBAU,UAAUA,CACRN,UAAkB,EAClBO,KAA4C,EAC5CX,OAA6B;IAE7B,OAAO,IAAI,CAACC,OAAO,CAACI,GAAG,CAAC,eAAeD,UAAU,SAAS,EAAE;MAC1DO,KAAK;MACLC,OAAO,EAAE,QAAQ;MACjB,GAAGZ,OAAO;MACVa,MAAM,EAAEF,KAAK,EAAEE,MAAM,IAAI;KAC1B,CAA+E;EAClF;;AAGF;;;AAGA,OAAM,MAAOL,aAAc,SAAQZ,IAAc;AA4UjD,WAAiBC,SAAS;EAIVA,SAAA,CAAAW,aAAa,GAAGb,YAAY,CAACa,aAAa;AAK1D,CAAC,EATgBX,SAAS,KAATA,SAAS"},"metadata":{},"sourceType":"module","externalDependencies":[]}